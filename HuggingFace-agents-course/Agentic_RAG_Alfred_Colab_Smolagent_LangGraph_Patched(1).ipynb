{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daJOOCSrGCbo"
      },
      "source": [
        "# 🎩 Alfred 晚宴 AI 教學筆記本（SmolAgents + LangGraph 版本）\n",
        "歡迎來到這個實戰導向的筆記本，我們將一起建構一位智慧代理人 Alfred，協助你處理一場奢華晚宴的大小事！\n",
        "\n",
        "這份筆記本提供 **雙版本實作範例**：\n",
        "- 🤖 SmolAgents：快速上手的工具驅動 Agent 架構\n",
        "- 🌐 LangGraph：可擴展流程圖式多工具 AI Agent\n",
        "\n",
        "你可以依需求選擇任一版本進行實作，或兩個都試試！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUhu3aKGCbq"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ✅ 安裝必要套件\n",
        "!pip install smolagents langchain langgraph huggingface_hub duckduckgo-search datasets langchain_community llama_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rank_bm25"
      ],
      "metadata": {
        "id": "AULLWqkDGcVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3241cj0xGCbq"
      },
      "source": [
        "## 🤖 SmolAgents 版本：快速構建多工具代理人 Alfred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_mOx8vuGCbr"
      },
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# 工具定義（Web Search / Weather / HF Stats / Guest Info RAG）\n",
        "from smolagents import CodeAgent, HfApiModel, Tool\n",
        "from huggingface_hub import list_models\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.docstore.document import Document\n",
        "from duckduckgo_search import DDGS\n",
        "import datasets, random\n",
        "\n",
        "# DuckDuckGo Search Tool\n",
        "class DuckDuckGoSearchTool(Tool):\n",
        "    name = \"web_search\"\n",
        "    description = \"Web search using DuckDuckGo\"\n",
        "    inputs = {\"query\": {\"type\": \"string\", \"description\": \"Web search query\"}}\n",
        "    output_type = \"string\"\n",
        "    def forward(self, query):\n",
        "        with DDGS() as ddgs:\n",
        "            results = ddgs.text(query, max_results=1)\n",
        "            return results[0]['body'] if results else \"No results found.\"\n",
        "\n",
        "# Weather Tool\n",
        "class WeatherInfoTool(Tool):\n",
        "    name = \"weather_info\"\n",
        "    description = \"Returns dummy weather info for a location\"\n",
        "    inputs = {\"location\": {\"type\": \"string\", \"description\": \"Location name\"}}\n",
        "    output_type = \"string\"\n",
        "    def forward(self, location):\n",
        "        data = random.choice([\n",
        "            {\"condition\": \"Sunny\", \"temp_c\": 28},\n",
        "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "            {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "        ])\n",
        "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
        "\n",
        "# HF Hub Stats Tool\n",
        "class HubStatsTool(Tool):\n",
        "    name = \"hub_stats\"\n",
        "    description = \"Fetches most popular model from HuggingFace for a given author\"\n",
        "    inputs = {\"author\": {\"type\": \"string\", \"description\": \"Author or org name\"}}\n",
        "    output_type = \"string\"\n",
        "    def forward(self, author):\n",
        "        try:\n",
        "            models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
        "            if models:\n",
        "                m = models[0]\n",
        "                return f\"The most downloaded model by {author} is {m.id} with {m.downloads:,} downloads.\"\n",
        "            return f\"No models found for {author}.\"\n",
        "        except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "# Guest Info Tool\n",
        "class GuestInfoRetrieverTool(Tool):\n",
        "    name = \"guest_info\"\n",
        "    description = \"Retrieve guest information by name or relation\"\n",
        "    inputs = {\"query\": {\"type\": \"string\", \"description\": \"The name or relation of the guest.\"}}\n",
        "    output_type = \"string\"\n",
        "    def __init__(self):\n",
        "        dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
        "        docs = [Document(\n",
        "            page_content=f\"Name: {g['name']}\\nRelation: {g['relation']}\\nDescription: {g['description']}\\nEmail: {g['email']}\",\n",
        "            metadata={\"name\": g[\"name\"]}) for g in dataset]\n",
        "        self.retriever = BM25Retriever.from_documents(docs)\n",
        "    def forward(self, query):\n",
        "        results = self.retriever.get_relevant_documents(query)\n",
        "        return \"\\n\\n\".join([r.page_content for r in results[:2]]) if results else \"No guest info found.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfX6e9x1GCbs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 建立 Alfred（SmolAgents）\n",
        "model = HfApiModel()\n",
        "alfred_smol = CodeAgent(\n",
        "    tools=[DuckDuckGoSearchTool(), WeatherInfoTool(), HubStatsTool(), GuestInfoRetrieverTool()],\n",
        "    model=model,\n",
        "    add_base_tools=True,\n",
        "    planning_interval=3\n",
        ")\n",
        "\n",
        "# 測試 Alfred\n",
        "query = \"Tell me about Lady Ada Lovelace\"\n",
        "print(\"🎩 Alfred 回應：\\n\", alfred_smol.run(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔨 實作：打造 Guestbook 工具**\n",
        "\n",
        "### **📍 Step 1：載入與轉換資料**\n",
        "\n",
        "我們會用 Hugging Face datasets 套件載入資料，並轉成 LangChain 可用的 Document 格式。"
      ],
      "metadata": {
        "id": "kRqoTBZhInPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
        "\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"\\n\".join([\n",
        "            f\"Name: {guest['name']}\",\n",
        "            f\"Relation: {guest['relation']}\",\n",
        "            f\"Description: {guest['description']}\",\n",
        "            f\"Email: {guest['email']}\"\n",
        "        ]),\n",
        "        metadata={\"name\": guest[\"name\"]}\n",
        "    )\n",
        "    for guest in guest_dataset\n",
        "]"
      ],
      "metadata": {
        "id": "WaFVzybaIj2k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUDFOFE-IqJd",
        "outputId": "c4518dc7-65ea-46a3-83b8-ea54f903bde1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'name': 'Ada Lovelace'}, page_content=\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\"),\n",
              " Document(metadata={'name': 'Dr. Nikola Tesla'}, page_content=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\"),\n",
              " Document(metadata={'name': 'Marie Curie'}, page_content='Name: Marie Curie\\nRelation: no relation\\nDescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nEmail: marie.curie@example.com')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "這段程式碼的功能是：\n",
        "\n",
        "- 載入資料集\n",
        "- 將每位來賓轉成一份 Document 物件\n",
        "- 每份 Document 包含格式化後的來賓資訊"
      ],
      "metadata": {
        "id": "RIUlUNMrIzCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔍 Step 2：建立檢索工具**\n",
        "\n",
        "我們用 BM25Retriever 來做文字搜尋，這是一種無需 embedding 的傳統 IR 方法，簡單又實用。"
      ],
      "metadata": {
        "id": "Dc6Mpw_5I1kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "class GuestInfoRetrieverTool(Tool):\n",
        "    name = \"guest_info_retriever\"\n",
        "    description = \"Retrieves detailed information about gala guests based on their name or relation.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The name or relation of the guest you want information about.\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, docs):\n",
        "        self.is_initialized = False\n",
        "        self.retriever = BM25Retriever.from_documents(docs)\n",
        "\n",
        "    def forward(self, query: str):\n",
        "        results = self.retriever.get_relevant_documents(query)\n",
        "        if results:\n",
        "            return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
        "        else:\n",
        "            return \"No matching guest information found.\""
      ],
      "metadata": {
        "id": "CLZ2WyocIsqM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化工具\n",
        "guest_info_tool = GuestInfoRetrieverTool(docs)"
      ],
      "metadata": {
        "id": "hAl_rvlvJlA8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🤖 Step 3：讓 Alfred 用起來！**\n",
        "\n",
        "將工具整合進 Alfred 的代理人架構中，讓他能根據語音或指令即時查詢來賓資訊："
      ],
      "metadata": {
        "id": "ijqVuWB0I8y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, HfApiModel\n",
        "\n",
        "model = HfApiModel()\n",
        "alfred = CodeAgent(tools=[guest_info_tool], model=model)\n",
        "\n",
        "response = alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\")\n",
        "\n",
        "print(\"🎩 Alfred's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rXJNxJ8hIsmJ",
        "outputId": "e940efe9-3319-4d64-f1b1-5177fa140bb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mTell me about our guest named 'Lady Ada Lovelace'.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Tell me about our guest named 'Lady Ada Lovelace'.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mlady_ada_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mguest_info_retriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLady Ada Lovelace\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlady_ada_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">lady_ada_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> guest_info_retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Lady Ada Lovelace\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(lady_ada_info)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-425149ac6647>:20: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = self.retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "Name: Ada Lovelace\n",
              "Relation: best friend\n",
              "Description: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for \n",
              "her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work\n",
              "on Charles Babbage's Analytical Engine.\n",
              "Email: ada.lovelace@example.com\n",
              "\n",
              "Name: Marie Curie\n",
              "Relation: no relation\n",
              "Description: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\n",
              "Email: marie.curie@example.com\n",
              "\n",
              "Name: Dr. Nikola Tesla\n",
              "Relation: old friend from university days\n",
              "Description: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless \n",
              "energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about \n",
              "pigeons, so that might make for good small talk.\n",
              "Email: nikola.tesla@gmail.com\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "Name: Ada Lovelace\n",
              "Relation: best friend\n",
              "Description: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for \n",
              "her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work\n",
              "on Charles Babbage's Analytical Engine.\n",
              "Email: ada.lovelace@example.com\n",
              "\n",
              "Name: Marie Curie\n",
              "Relation: no relation\n",
              "Description: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\n",
              "Email: marie.curie@example.com\n",
              "\n",
              "Name: Dr. Nikola Tesla\n",
              "Relation: old friend from university days\n",
              "Description: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless \n",
              "energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about \n",
              "pigeons, so that might make for good small talk.\n",
              "Email: nikola.tesla@gmail.com\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 39.03 seconds| Input tokens: 2,081 | Output tokens: 65]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 39.03 seconds| Input tokens: 2,081 | Output tokens: 65]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mError in generating model output:\u001b[0m\n",
              "\u001b[1;36m500\u001b[0m\u001b[1;31m Server Error: Internal Server Error for url: \u001b[0m\n",
              "\u001b[4;94mhttps://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: \u001b[0m\n",
              "\u001b[1;33mRoot\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;31m-67f130f1-5c09f06a2892fb9f7ec84a06;\u001b[0m\u001b[93m5ae36838-d52d-4d2b-ba9b-b9bf5de61eed\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\n",
              "\u001b[1;31mModel too busy, unable to get response in less than \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;35msecond\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31ms\u001b[0m\u001b[1;31m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Server Error: Internal Server Error for url: </span>\n",
              "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> (Request ID: </span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Root</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-67f130f1-5c09f06a2892fb9f7ec84a06;</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">5ae36838-d52d-4d2b-ba9b-b9bf5de61eed</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Model too busy, unable to get response in less than </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">second</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(s)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 60.11 seconds| Input tokens: 4,162 | Output tokens: 130]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 60.11 seconds| Input tokens: 4,162 | Output tokens: 130]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AgentGenerationError",
          "evalue": "Error in generating model output:\n500 Server Error: Internal Server Error for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-67f130f1-5c09f06a2892fb9f7ec84a06;5ae36838-d52d-4d2b-ba9b-b9bf5de61eed)\n\nModel too busy, unable to get response in less than 60 second(s)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0madditional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             chat_message: ChatMessage = self.model(\n\u001b[0m\u001b[1;32m   1226\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         )\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    991\u001b[0m         )\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-67f130f1-5c09f06a2892fb9f7ec84a06;5ae36838-d52d-4d2b-ba9b-b9bf5de61eed)\n\nModel too busy, unable to get response in less than 60 second(s)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-72c62fea832b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malfred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodeAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mguest_info_tool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malfred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tell me about our guest named 'Lady Ada Lovelace'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎩 Alfred's Response:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# Outputs are returned only at the end. We only look at the last step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     def _run(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_action_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_start_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_execute_step\u001b[0;34m(self, task, memory_step)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_step\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActionStep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {self.step_number}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_answer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer_checks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_final_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0mmemory_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAgentGenerationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in generating model output:\\n{e}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         self.logger.log_markdown(\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m: Error in generating model output:\n500 Server Error: Internal Server Error for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-67f130f1-5c09f06a2892fb9f7ec84a06;5ae36838-d52d-4d2b-ba9b-b9bf5de61eed)\n\nModel too busy, unable to get response in less than 60 second(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web search on different Agent Frameworks"
      ],
      "metadata": {
        "id": "giivGJo8KrSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "class DuckSearchSummary:\n",
        "    def __call__(self, query):\n",
        "        with DDGS() as ddgs:\n",
        "            results = list(ddgs.text(query, max_results=1))\n",
        "            return results[0][\"body\"] if results else \"No result found.\"\n",
        "\n",
        "# 測試\n",
        "search_tool = MySimpleDuckSearch()\n",
        "# 先搜尋，再加一個 LLM 摘要步驟\n",
        "context = DuckSearchSummary()(\"Who is the current President of France?\")\n",
        "response = alfred.run(f\"Based on the following web content, tell me who is the current president of France:\\n\\n{context}\")\n",
        "print(\"🎩 Alfred's Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "wBunUpZ5Iscg",
        "outputId": "b7445ddd-53c9-4eb1-8f72-93a2f50a8be2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mBased on the following web content, tell me who is the current president of France:\u001b[0m                             \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mThe current president of France is Emmanuel Macron, who was re-elected in 2022 for a second term. Learn about \u001b[0m  \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mthe history, powers, functions and duties of the French presidency, and the electoral process and requirements.\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Based on the following web content, tell me who is the current president of France:</span>                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">The current president of France is Emmanuel Macron, who was re-elected in 2022 for a second term. Learn about </span>  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">the history, powers, functions and duties of the French presidency, and the electoral process and requirements.</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mcurrent_president\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mEmmanuel Macron\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcurrent_president\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">current_president </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Emmanuel Macron\"</span><span style=\"background-color: #272822\">                                                                          </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(current_president)</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mOut - Final answer: Emmanuel Macron\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: Emmanuel Macron</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 0.20 seconds| Input tokens: 2,131 | Output tokens: 61]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.20 seconds| Input tokens: 2,131 | Output tokens: 61]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎩 Alfred's Response:\n",
            "Emmanuel Macron\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "tool_spec = DuckDuckGoSearchToolSpec()\n",
        "search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)\n",
        "print(search_tool(\"Who's the current President of France?\").raw_output[-1]['body'])"
      ],
      "metadata": {
        "id": "OOfwtDhyK2iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "print(search_tool.invoke(\"Who's the current President of France?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YUQE94iMIrm",
        "outputId": "a289e572-4c29-4bcc-b24f-56797794de46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emmanuel Macron is a French banker and politician who was elected president of France in 2017. Macron was the first person in the history of the Fifth Republic to win the presidency without the backing of either the Socialists or the Gaullists, and he was France's youngest head of state since Napoleon. PARIS (AP) — French President Emmanuel Macron vowed Thursday to stay in office until the end of his term, due in 2027, and announced that he will name a new prime minister within days following ... Find out who the current president of France is, his political career, his actions, and his impact on the country. Stay informed about French news and presidential decisions. French President Emmanuel Macron came out fighting Thursday in his first comments following the resignation of ousted Prime Minister Michel Barnier, a day after a historic no-confidence vote at ... France has a semi-presidential system of government, offering almost a balanced power sharing between the president and the prime minister.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🎆 安排煙火要看天氣：Weather Tool 工具**\n",
        "\n",
        "煙火施放需要精準的天氣判斷。我們將建立一個簡單的 **天氣查詢工具**（使用虛擬 API 範例），讓 Alfred 根據地點判斷是否適合放煙火。"
      ],
      "metadata": {
        "id": "9JvXVI8ZMRSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#smolagent\n",
        "from smolagents import Tool\n",
        "import random\n",
        "\n",
        "class WeatherInfoTool(Tool):\n",
        "    name = \"weather_info\"\n",
        "    description = \"Fetches dummy weather information for a given location.\"\n",
        "    inputs = {\"location\": {\"type\": \"string\", \"description\": \"Location name\"}}\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, location: str):\n",
        "        weather_conditions = [\n",
        "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "            {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "            {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "        ]\n",
        "        data = random.choice(weather_conditions)\n",
        "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
        "\n",
        "weather_info_tool = WeatherInfoTool()"
      ],
      "metadata": {
        "id": "F6lDQ-edMRoE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#langchain\n",
        "from langchain.tools import Tool\n",
        "\n",
        "def get_weather_info(location: str) -> str:\n",
        "    weather_conditions = [...]\n",
        "    data = random.choice(weather_conditions)\n",
        "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
        "\n",
        "weather_info_tool = Tool(\n",
        "    name=\"get_weather_info\",\n",
        "    func=get_weather_info,\n",
        "    description=\"Fetches dummy weather information for a given location.\"\n",
        ")"
      ],
      "metadata": {
        "id": "S2UM9TdnMjtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🤖 Impress the AI Guests：Hugging Face Hub Stats 工具**\n",
        "\n",
        "這場晚宴雲集 AI 領域的巨頭人物。Alfred 要如何讓對話更有水準？當然要聊他們的模型啦！\n",
        "\n",
        "我們會建立一個工具，查詢指定作者在 Hugging Face 上最受歡迎的模型"
      ],
      "metadata": {
        "id": "Cv0LNetCNGbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "class HubStatsTool(Tool):\n",
        "    name = \"hub_stats\"\n",
        "    description = \"Fetches top downloaded model by author.\"\n",
        "    inputs = {\"author\": {\"type\": \"string\", \"description\": \"Author name\"}}\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, author: str):\n",
        "        try:\n",
        "            models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
        "            if models:\n",
        "                model = models[0]\n",
        "                return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
        "            else:\n",
        "                return f\"No models found for author {author}.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "hub_stats_tool = HubStatsTool()"
      ],
      "metadata": {
        "id": "jHHE4WgDMjqr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool\n",
        "\n",
        "def get_hub_stats(author: str) -> str:\n",
        "    models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
        "    if models:\n",
        "        model = models[0]\n",
        "        return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
        "    return f\"No models found.\"\n",
        "\n",
        "hub_stats_tool = Tool(\n",
        "    name=\"get_hub_stats\",\n",
        "    func=get_hub_stats,\n",
        "    description=\"Fetches Hugging Face model stats by author.\"\n",
        ")"
      ],
      "metadata": {
        "id": "3eL6T5qJMjfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🤝 整合所有工具進 Alfred！**\n",
        "\n",
        "現在我們有三大工具：Web Search、天氣、Hugging Face 統計。讓我們一起整合進 Alfred 的代理人模型中："
      ],
      "metadata": {
        "id": "TudE3b3DNPhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, HfApiModel\n",
        "from smolagents import Tool  # ✅ 必須是 smolagents 提供的\n",
        "\n",
        "from smolagents import Tool\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "class DuckSearchTool(Tool):\n",
        "    name = \"web_search\"\n",
        "    description = \"Search the web using DuckDuckGo\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Query to search on DuckDuckGo\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, query: str):\n",
        "        with DDGS() as ddgs:\n",
        "            results = list(ddgs.text(query, max_results=1))\n",
        "            return results[0]['body'] if results else \"No result found.\"\n",
        "search_tool = DuckSearchTool()  # ✅ 必須是類別實例！\n",
        "\n",
        "class WeatherInfoTool(Tool):\n",
        "    name = \"weather_info\"\n",
        "    description = \"Fetches dummy weather info.\"\n",
        "    inputs = {\n",
        "        \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The location to get weather info for.\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, location: str):\n",
        "        import random\n",
        "        options = [\"Sunny\", \"Rainy\", \"Windy\"]\n",
        "        return f\"The weather in {location} is {random.choice(options)}.\"\n",
        "\n",
        "weather_info_tool = WeatherInfoTool()\n",
        "\n",
        "from smolagents import Tool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "class HubStatsTool(Tool):\n",
        "    name = \"hub_stats\"\n",
        "    description = \"Get top model for a HF author\"\n",
        "    inputs = {\n",
        "        \"author\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"HuggingFace username\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, author: str):\n",
        "        try:\n",
        "            models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
        "            if models:\n",
        "                m = models[0]\n",
        "                return f\"Top model: {m.id} with {m.downloads:,} downloads.\"\n",
        "            return \"No models found.\"\n",
        "        except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "hub_stats_tool = HubStatsTool()\n",
        "\n",
        "assert isinstance(search_tool, Tool)\n",
        "assert isinstance(weather_info_tool, Tool)\n",
        "assert isinstance(hub_stats_tool, Tool)"
      ],
      "metadata": {
        "id": "i-OjzXeZNPw0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HfApiModel()\n",
        "alfred = CodeAgent(\n",
        "    tools=[search_tool, weather_info_tool, hub_stats_tool],\n",
        "    model=model\n",
        ")\n",
        "# response = alfred.run(\"What is Facebook and what's their most popular model?\")\n",
        "# print(\"🎩 Alfred's Response:\", response)"
      ],
      "metadata": {
        "id": "aXDI4mWzOak1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SmolAgents 版本 來看看這個多功能的Agent可不可以回答下面的隨機問題！？"
      ],
      "metadata": {
        "id": "RFMiSzsXO9ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\"\n",
        "response = alfred.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "collapsed": true,
        "id": "U3LmVfjTOttu",
        "outputId": "44a2b65c-78cf-4427-cfac-1b637ab19754"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat's the weather like in Paris tonight? Will it be suitable for our fireworks display?\u001b[0m                        \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What's the weather like in Paris tonight? Will it be suitable for our fireworks display?</span>                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mError in generating model output:\u001b[0m\n",
              "\u001b[1;36m402\u001b[0m\u001b[1;31m Client Error: Payment Required for url: \u001b[0m\n",
              "\u001b[4;94mhttps://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: \u001b[0m\n",
              "\u001b[1;33mRoot\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;31m-67f136a7-4227a87b1376d4e565b2686f;\u001b[0m\u001b[93m90bcfc03-9a42-4571-a355-faad37e6f42a\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\n",
              "\u001b[1;31mYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 2\u001b[0m\u001b[1;36m0x\u001b[0m\u001b[1;31m more monthly \u001b[0m\n",
              "\u001b[1;31mincluded credits.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Client Error: Payment Required for url: </span>\n",
              "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> (Request ID: </span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Root</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-67f136a7-4227a87b1376d4e565b2686f;</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">90bcfc03-9a42-4571-a355-faad37e6f42a</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 2</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> more monthly </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">included credits.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 0.05 seconds]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.05 seconds]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AgentGenerationError",
          "evalue": "Error in generating model output:\n402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-67f136a7-4227a87b1376d4e565b2686f;90bcfc03-9a42-4571-a355-faad37e6f42a)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0madditional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             chat_message: ChatMessage = self.model(\n\u001b[0m\u001b[1;32m   1226\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         )\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    991\u001b[0m         )\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-67f136a7-4227a87b1376d4e565b2686f;90bcfc03-9a42-4571-a355-faad37e6f42a)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-72fbe0b367ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malfred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# Outputs are returned only at the end. We only look at the last step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     def _run(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_action_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_start_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_execute_step\u001b[0;34m(self, task, memory_step)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_step\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActionStep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {self.step_number}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_answer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer_checks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_final_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0mmemory_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAgentGenerationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in generating model output:\\n{e}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         self.logger.log_markdown(\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m: Error in generating model output:\n402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-67f136a7-4227a87b1376d4e565b2686f;90bcfc03-9a42-4571-a355-faad37e6f42a)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}