{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching\n",
    "\n",
    "## Overview\n",
    "\n",
    "Prompt caching is a 強大的功能，可以通過在您的提示中啟用特定前綴的恢復來優化 API 使用。  \n",
    "此方法大大降低了重複任務或具有一致成分的提示的處理時間和成本。\n",
    "\n",
    "Prompt Caching 對於以下情況特別有用:\n",
    "\n",
    "- 具有多個示例的提示\n",
    "- 具有大量上下文或背景信息的提示\n",
    "- 具有一致指示的重複任務\n",
    "- 長時間多輪對話\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Fetch Data](#fetch-data)\n",
    "- [OpenAI](#openai)\n",
    "- [Anthropic](#anthropic)\n",
    "- [GoogleAI](#googleai)\n",
    "\n",
    "### References\n",
    "\n",
    "- [OpenAI Prompt Caching Documentation](https://platform.openai.com/docs/guides/prompt-caching)\n",
    "- [Anthropic Prompt Caching Documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)\n",
    "- [Google Gemini API - Context Caching](https://ai.google.dev/gemini-api/docs/caching)\n",
    "- [LangChain Google Generative AI - ChatGoogleGenerativeAI](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html#chatgooglegenerativeai)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain-core\",\n",
    "        \"langchain-openai\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain-google-genai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {   \n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"ANTHROPIC_API_KEY\": \"\",\n",
    "        \"GOOGLE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Prompt-Caching\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "\n",
    "最簡單的方法來驗證提示緩存是通過包含大量上下文或背景信息。  \n",
    "為了示範這一點，我提供了一個簡單的示例，使用從 Wikipedia 擷取的長文檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def fetch_wikipedia_page(title: str, lang: str = \"en\"):\n",
    "    \"\"\"\n",
    "    抓取 the content of a Wikipedia page using the Wikipedia API.\n",
    "    \n",
    "    Args:\n",
    "        title (str): The title of the Wikipedia page to fetch.\n",
    "        lang (str): The language code for the Wikipedia (default: \"en\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The plain text content of the Wikipedia page.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    endpoint = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    # Query parameters\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": title,\n",
    "        \"explaintext\": True\n",
    "    }\n",
    "    \n",
    "    # Encode the parameters and create the URL\n",
    "    url = f\"{endpoint}?{urllib.parse.urlencode(params)}\"\n",
    "    \n",
    "    # Send the request and read the response\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        data = json.load(response)\n",
    "    \n",
    "    # Extract page content\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for page_id, page in pages.items():\n",
    "        if \"extract\" in page:\n",
    "            return page[\"extract\"]\n",
    "    \n",
    "    return \"No content found for the given title.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from wikipedia\n",
    "title = \"World War II\"\n",
    "content = fetch_wikipedia_page(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI 提示詞快取\n",
    "\n",
    "OpenAI 提示詞快取功能會自動應用於所有 API 請求（無需修改代碼），且不收取額外費用。  \n",
    "對於長提示詞，這可以將延遲降低高達 **80%**，並節省 **50%** 的成本。此功能適用於包含 1024 個或更多 token 的提示詞。\n",
    "\n",
    "### 支援提示詞快取的模型\n",
    "\n",
    "| 模型                                    | 文字輸入成本 | 語音輸入成本 |\n",
    "|----------------------------------------|--------------|--------------|\n",
    "| gpt-4o (不包含 gpt-4o-2024-05-13 和 chatgpt-4o-latest) | 減少 50%     | 不適用       |\n",
    "| gpt-4o-mini                            | 減少 50%     | 不適用       |\n",
    "| gpt-4o-realtime-preview                | 減少 50%     | 減少 80%     |\n",
    "| o1-preview                             | 減少 50%     | 不適用       |\n",
    "| o1-mini                                | 減少 50%     | 不適用       |\n",
    "\n",
    "如需詳細資訊，請參考以下連結：  \n",
    "[OpenAI 提示詞快取](https://platform.openai.com/docs/guides/prompt-caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Australia and New Zealand joined World War II shortly after the outbreak of the war in Europe. Both countries declared war on Germany on 3 September 1939, following the United Kingdom's declaration of war on Germany after the invasion of Poland.\n",
      "Token Usage: {'token_usage': {'completion_tokens': 49, 'prompt_tokens': 17389, 'total_tokens': 17438, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "Caching Answer: The first battle between Australia, New Zealand, and Japan took place at the Battle of Rabaul, which occurred in January 1942. This battle was part of the broader conflict in the Pacific during World War II.\n",
      "Token Usage: {'token_usage': {'completion_tokens': 46, 'prompt_tokens': 17395, 'total_tokens': 17441, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 17152}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            #The {content} is sourced from the Wikipedia article mentioned above.\n",
    "            \"You are an assistant who answers questions based on the provided document.\\n<document>{content}</document>\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{question}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "first_response = chain.invoke({\"content\": content,\"question\":\"When did Australia and New Zealand join the war?\"})\n",
    "second_response = chain.invoke({\"content\": content,\"question\":\"Where did the first battle between Australia, New Zealand, and Japan take place?\"})\n",
    "\n",
    "# You can see only cache read in 'prompt_tokens_details' -> 'cached_tokens' in langchain 0.3.29 OpenAI calls.\n",
    "print(f\"Answer: {first_response.content}\")\n",
    "print(f\"Token Usage: {first_response.response_metadata}\")\n",
    "print()\n",
    "print(f\"Caching Answer: {second_response.content}\")\n",
    "print(f\"Token Usage: {second_response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "\n",
    "Anthropic 提示詞快取提供以下 token 限制：\n",
    "- **1024 tokens** for Claude 3.5 Sonnet and Claude 3 Opus\n",
    "- **2048 tokens** for Claude 3.5 Haiku and Claude 3 Haiku\n",
    "\n",
    "**[注意]**\n",
    "- 短提示詞無法快取，即使標記為 ```cache_control```。\n",
    "- 快取有 **5 分鐘的存活時間 (TTL)**。目前，```ephemeral``` 是唯一支持的快取類型，對應於此 5 分鐘的存活時間。\n",
    "\n",
    "### 支援提示詞快取的模型\n",
    "- Claude 3.5 Sonnet\n",
    "- Claude 3.5 Haiku\n",
    "- Claude 3 Haiku\n",
    "- Claude 3 Opus\n",
    "\n",
    "雖然它需要遵守 Anthropic Message Style，但 Anthropic 提示詞快取的一大優點是它能以更少的 token 進行快取。  \n",
    "\n",
    "如需詳細資訊，請點擊以下連結：   \n",
    "[Anthropic 提示詞快取](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: According to the document, Yugoslavia was invaded by Germany and Italy as part of their broader operations in the Balkans. The specific details are mentioned in this passage:\n",
      "\n",
      "\"By late March 1941, Bulgaria and Yugoslavia signed the Tripartite Pact; however, the Yugoslav government was overthrown two days later by pro-British nationalists. Germany and Italy responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month.\"\n",
      "\n",
      "The invasion appears to have been a response to the overthrow of the government that had previously signed the Tripartite Pact. Germany and Italy saw this as a threat to their strategic interests in the region and quickly moved to occupy Yugoslavia. After the invasion, partisan warfare broke out against the Axis occupation, which continued until the end of the war.\n",
      "Token Usage: {'id': 'msg_01N6edkmZ6NGT5RmZs85uFya', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 18837, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 186}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model = \"claude-3-5-haiku-latest\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            #The {content} is sourced from the Wikipedia article mentioned above.\n",
    "            \"text\": f\"You are an assistant who answers questions based on the provided document.\\n<document>{content}</document>\", \n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Why was Yugoslavia invaded?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "first_response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Answer: {first_response.content}\")\n",
    "# You can see cache read in 'input_token_details' -> 'cache_creation_tokens' or 'cache_read_input_tokens'.\n",
    "print(f\"Token Usage: {first_response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: According to the document, after Yugoslavia was invaded by Germany and Italy, Greece was also invaded. Specifically, the text states: \"Germany and Italy responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month.\"\n",
      "Token Usage: {'id': 'msg_019t8wXVpXpYbasNRb7WBrsv', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 18837, 'input_tokens': 13, 'output_tokens': 66}}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            #The {content} is sourced from the Wikipedia article mentioned above.\n",
    "            \"text\": f\"You are an assistant who answers questions based on the provided document.\\n<document>{content}</document>\", \n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Where was invaded after Yugoslavia?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "second_response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Answer: {second_response.content}\")\n",
    "# You can see cache read in 'input_token_details' -> 'cache_creation_tokens' or 'cache_read_input_tokens'.\n",
    "print(f\"Token Usage: {second_response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleAI\n",
    "\n",
    "Google 將其稱為 Context Caching，而不是 Prompt Caching，並主要用於分析各種數據類型，如代碼分析、大型文檔集合、長視頻和多個音頻文件。\n",
    "\n",
    "因此，我們將展示如何使用 ```google.generativeai``` 中的 ```ChatGoogleGenerativeAI``` 來演示 ```langchain_google_genai```。\n",
    "\n",
    "有關更多信息，請參閱以下連結：  \n",
    "- [Google Gemini API - Context Caching](https://ai.google.dev/gemini-api/docs/caching)\n",
    "- [LangChain Google Generative AI - ChatGoogleGenerativeAI](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html#chatgooglegenerativeai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data For GoogleAI Context Caching\n",
    "\n",
    "最少需要 **32,768** tokens for Prompt Caching (which Google refers to as Context Caching).   \n",
    "因此，我們決定以簡單的方式實現並通過包含三個長篇 Wikipedia 文件來演示其用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_featured_list_in_wikipedia = \"List of Falcon 9 and Falcon Heavy launches\"\n",
    "falcon_wiki = fetch_wikipedia_page(longest_featured_list_in_wikipedia)\n",
    "\n",
    "longest_biography_in_wikipedia = \"Vladimir Putin\"\n",
    "putin_wiki = fetch_wikipedia_page(longest_biography_in_wikipedia)\n",
    "\n",
    "python_wiki_page = \"Python (programming language)\"\n",
    "python_wiki = fetch_wikipedia_page(python_wiki_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "\n",
    "cache = caching.CachedContent.create(\n",
    "    model='models/gemini-1.5-flash-001',\n",
    "    display_name='wikipedia-document-pages', # used to identify the cache.\n",
    "    system_instruction=(\n",
    "        'You are an expert in analyze very long text, and your job is to answer '\n",
    "        'the user\\'s query based on the video file you have access to.'\n",
    "    ), # if long, complex system instruction needed, you can provide with this format.\n",
    "    contents=[falcon_wiki, putin_wiki, python_wiki], # you can pass each documents in list format.\n",
    "    ttl=datetime.timedelta(minutes=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CachedContent(\n",
      "    name='cachedContents/7odha6ycbqsi',\n",
      "    model='models/gemini-1.5-flash-001',\n",
      "    display_name='wikipedia-document-pages',\n",
      "    usage_metadata={\n",
      "        'total_token_count': 43394,\n",
      "    },\n",
      "    create_time=2025-02-04 17:59:43.621411+00:00,\n",
      "    update_time=2025-02-04 17:59:43.621411+00:00,\n",
      "    expire_time=2025-02-04 18:04:30.411653+00:00\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cache) # When caching, the model name provided must be the same when creating an instance of ChatGoogleGenerativeAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text states that as of February 4th, 2025, SpaceX has conducted **15** Falcon family launches in 2025. All of these launches were conducted using the Falcon 9 rocket, with no Falcon Heavy launches. \n",
      "\n",
      "{'input_tokens': 43408, 'output_tokens': 53, 'total_tokens': 43461, 'input_token_details': {'cache_read': 43394}}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-001\", cached_content=cache.name) # provide cache's name parameter for trackability.\n",
    "response = llm.invoke(\"How many Falcon Rockets launch conducted in 2025?\")\n",
    "\n",
    "print(response.content) \n",
    "print(response.usage_metadata) # you can see 'input_token_details' actually works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
